#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Investment Masters RAG Integration Example

ä½¿ç”¨ LangChain + ChromaDB å®ç°æŠ•èµ„å¤§å¸ˆçŸ¥è¯†åº“çš„ RAG æ£€ç´¢ã€‚

Requirements:
    pip install langchain langchain-community chromadb pyyaml sentence-transformers

Usage:
    python rag_langchain.py "è¿™ä¸ªè‚¡ç¥¨å€¼å¾—ä¹°å—ï¼Ÿ"
    python rag_langchain.py --interactive
    python rag_langchain.py --persist ./vectorstore "æŠ¤åŸæ²³åˆ†æ"
    python rag_langchain.py --load ./vectorstore "å·´è²ç‰¹å¦‚ä½•é€‰è‚¡ï¼Ÿ"
    python rag_langchain.py "æ­¢æŸ" --kind risk_management
    python rag_langchain.py "æŠ¤åŸæ²³" --investor warren_buffett
"""

import argparse
import os
import sys
from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    missing = []
    
    try:
        import yaml
    except ImportError:
        missing.append("pyyaml")
    
    try:
        from langchain.schema import Document
    except ImportError:
        missing.append("langchain")
    
    try:
        import chromadb
    except ImportError:
        missing.append("chromadb")

    # HuggingFaceEmbeddings é»˜è®¤ä¾èµ– sentence-transformers
    try:
        import sentence_transformers  # noqa: F401
    except ImportError:
        missing.append("sentence-transformers")
    
    if missing:
        print("ç¼ºå°‘ä¾èµ–ï¼Œè¯·å®‰è£…:")
        print(f"  pip install {' '.join(missing)}")
        sys.exit(1)


def load_investor_documents():
    """åŠ è½½æŠ•èµ„è€…æ–‡æ¡£ä¸º LangChain Document æ ¼å¼"""
    from langchain.schema import Document
    import yaml
    
    documents = []
    investors_dir = PROJECT_ROOT / "investors"
    
    # åŠ è½½æŠ•èµ„è€…ç´¢å¼•è·å–å…ƒæ•°æ®
    index_file = PROJECT_ROOT / "config" / "investor_index.yaml"
    investor_meta = {}
    
    if index_file.exists():
        with open(index_file, "r", encoding="utf-8") as f:
            index_data = yaml.safe_load(f)
            for inv in index_data.get("investors", []):
                investor_meta[inv["id"]] = inv
    
    # åŠ è½½æ¯ä¸ªæŠ•èµ„è€…çš„ Markdown æ–‡ä»¶
    for md_file in investors_dir.glob("*.md"):
        investor_id = md_file.stem
        
        with open(md_file, "r", encoding="utf-8") as f:
            content = f.read()
        
        # è·å–å…ƒæ•°æ®
        meta = investor_meta.get(investor_id, {})
        
        doc = Document(
            page_content=content,
            metadata={
                "source": str(md_file.relative_to(PROJECT_ROOT)),
                "investor_id": investor_id,
                "investor_name": meta.get("full_name", investor_id),
                "chinese_name": meta.get("chinese_name", ""),
                "style": ", ".join(meta.get("style", [])),
                "best_for": ", ".join(meta.get("best_for", [])),
            }
        )
        documents.append(doc)
    
    return documents


def split_investor_documents(documents, chunk_size: int = 900, chunk_overlap: int = 200):
    """
    å°†æŠ•èµ„è€…é•¿æ–‡æ¡£åˆ†å—ï¼Œæå‡æ£€ç´¢ç²¾åº¦ï¼Œå¹¶ä¸ºæ¯ä¸ªå—é™„åŠ å¼•ç”¨ä¿¡æ¯ã€‚

    - ä¿ç•™åŸ metadataï¼ˆsource/investor_id ç­‰ï¼‰
    - å¢åŠ  chunk_index/chunk_id/title_hint/source_type
    - è®°å½• start_index ç”¨äºç²¾ç¡®æº¯æº
    """
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    import re

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n## ", "\n### ", "\n#### ", "\n\n", "\n", " "],
        add_start_index=True,
    )

    split_docs = []

    for parent in documents:
        investor_id = parent.metadata.get("investor_id", "unknown")
        chunks = splitter.split_documents([parent])

        for idx, doc in enumerate(chunks):
            # æ ‡é¢˜æç¤ºï¼šå– chunk å†…ç¬¬ä¸€ä¸ª markdown æ ‡é¢˜
            m = re.search(r"(?m)^(#{1,4})\s+(.+?)\s*$", doc.page_content)
            title_hint = m.group(2) if m else ""

            doc.metadata["source_type"] = "investor_doc"
            doc.metadata["chunk_index"] = idx
            doc.metadata["chunk_id"] = f"{investor_id}#{idx}"
            if title_hint:
                doc.metadata["title_hint"] = title_hint

        split_docs.extend(chunks)

    return split_docs


def load_decision_rules():
    """åŠ è½½å†³ç­–è§„åˆ™ä¸º Document æ ¼å¼"""
    from langchain.schema import Document
    import json
    
    rules_file = PROJECT_ROOT / "config" / "decision_rules.generated.json"
    
    if not rules_file.exists():
        print(f"è§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨: {rules_file}")
        return []
    
    with open(rules_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    documents = []
    for rule in data.get("rules", []):
        # å°†è§„åˆ™è½¬ä¸ºè‡ªç„¶è¯­è¨€
        content = f"""
æŠ•èµ„è€…: {rule.get('investor_id', 'unknown')}
è§„åˆ™ç±»å‹: {rule.get('kind', 'other')}

IF {rule.get('when', 'N/A')}
THEN {rule.get('then', 'N/A')}
BECAUSE {rule.get('because', 'N/A')}
        """.strip()
        
        doc = Document(
            page_content=content,
            metadata={
                "source": "decision_rules.generated.json",
                "investor_id": rule.get("investor_id", "unknown"),
                "rule_id": rule.get("rule_id", ""),
                "kind": rule.get("kind", "other"),
                "source_type": "rule",
            }
        )
        documents.append(doc)
    
    return documents


def create_vectorstore(documents, persist_dir=None):
    """åˆ›å»ºå‘é‡å­˜å‚¨"""
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import Chroma
    
    # ä½¿ç”¨å…è´¹çš„æœ¬åœ° embedding æ¨¡å‹
    # å¦‚æœæœ‰ OpenAI API keyï¼Œå¯ä»¥æ”¹ç”¨ OpenAIEmbeddings
    print("åŠ è½½ embedding æ¨¡å‹...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"}
    )
    
    print(f"åˆ›å»ºå‘é‡å­˜å‚¨ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£...")
    
    if persist_dir:
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=persist_dir
        )
        vectorstore.persist()
    else:
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings
        )
    
    return vectorstore


def load_vectorstore(persist_dir: str):
    """ä»æŒä¹…åŒ–ç›®å½•åŠ è½½å‘é‡å­˜å‚¨ï¼ˆéœ€ä¸åˆ›å»ºæ—¶ä½¿ç”¨åŒä¸€ embedding é…ç½®ï¼‰"""
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import Chroma

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
    )

    return Chroma(
        persist_directory=persist_dir,
        embedding_function=embeddings,
    )


def query_vectorstore(vectorstore, query: str, k: int = 5, filter_dict: dict = None):
    """æŸ¥è¯¢å‘é‡å­˜å‚¨ï¼Œæ”¯æŒå…ƒæ•°æ®è¿‡æ»¤"""
    # Chroma è¿‡æ»¤è¯­æ³•ï¼š{"metadata_key": "value"} æˆ– {"$and": [...]}
    results = vectorstore.similarity_search_with_score(
        query, 
        k=k,
        filter=filter_dict
    )
    return results


def format_results(results):
    """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
    output = []
    
    for i, (doc, score) in enumerate(results, 1):
        investor_id = doc.metadata.get("investor_id", "unknown")
        investor_name = doc.metadata.get("chinese_name") or doc.metadata.get("investor_name") or investor_id
        source = doc.metadata.get("source", "unknown")
        source_type = doc.metadata.get("source_type", "unknown")
        rule_id = doc.metadata.get("rule_id", "")
        chunk_id = doc.metadata.get("chunk_id", "")
        title_hint = doc.metadata.get("title_hint", "")
        start_index = doc.metadata.get("start_index", 0)

        # å¼•ç”¨ï¼šä¼˜å…ˆ rule_idï¼Œå…¶æ¬¡ chunk_id
        citation = rule_id or chunk_id or "N/A"
        
        output.append(f"\n{'='*60}")
        output.append(f"[{i}] ç›¸ä¼¼åº¦(ä¼°ç®—): {1-score:.2%} | ç±»å‹: {source_type} | æ¥æº: {source}")
        output.append(f"    æŠ•èµ„è€…: {investor_name} ({investor_id})")
        if title_hint:
            output.append(f"    ç« èŠ‚: {title_hint}")
        if source_type == "investor_doc":
            output.append(f"    ä½ç½®: å­—ç¬¦åç§» {start_index}")
        output.append(f"    å¼•ç”¨: {citation}")
        output.append("-" * 60)
        
        # æˆªå–å†…å®¹é¢„è§ˆ
        content = doc.page_content[:500]
        if len(doc.page_content) > 500:
            content += "..."
        output.append(content)
        output.append(f"\nğŸ“Œ å¯æº¯æºå¼•ç”¨: {source}  ->  {citation} (offset: {start_index})")
    
    return "\n".join(output)


def interactive_mode(vectorstore, filter_dict=None):
    """äº¤äº’æ¨¡å¼"""
    print("\n" + "=" * 60)
    print("æŠ•èµ„å¤§å¸ˆçŸ¥è¯†åº“ - äº¤äº’æŸ¥è¯¢æ¨¡å¼")
    if filter_dict:
        print(f"æ´»åŠ¨è¿‡æ»¤å™¨: {filter_dict}")
    print("è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("=" * 60)
    
    while True:
        try:
            query = input("\nğŸ” ä½ çš„é—®é¢˜: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nå†è§!")
            break
        
        if query.lower() in ["quit", "exit", "q"]:
            print("å†è§!")
            break
        
        if not query:
            continue
        
        results = query_vectorstore(vectorstore, query, filter_dict=filter_dict)
        print(format_results(results))


def main():
    parser = argparse.ArgumentParser(
        description="Investment Masters RAG Query",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s "è¿™ä¸ªè‚¡ç¥¨å€¼å¾—ä¹°å—ï¼Ÿ"
  %(prog)s "å¸‚åœºææ…Œæ—¶è¯¥æ€ä¹ˆåŠï¼Ÿ"
  %(prog)s --interactive
  %(prog)s --persist ./vectorstore "æŠ¤åŸæ²³åˆ†æ"
  %(prog)s --load ./vectorstore "èŠ’æ ¼çš„å†³ç­–æ¸…å•"
        """
    )
    
    parser.add_argument(
        "query",
        nargs="?",
        help="æŸ¥è¯¢é—®é¢˜"
    )
    parser.add_argument(
        "--interactive", "-i",
        action="store_true",
        help="äº¤äº’æ¨¡å¼"
    )
    parser.add_argument(
        "--persist", "-p",
        help="å‘é‡å­˜å‚¨æŒä¹…åŒ–ç›®å½•"
    )
    parser.add_argument(
        "--load", "-l",
        help="åŠ è½½å·²ä¿å­˜çš„å‘é‡å­˜å‚¨ç›®å½•ï¼ˆæ›´å¿«ï¼‰"
    )
    parser.add_argument(
        "--top-k", "-k",
        type=int,
        default=5,
        help="è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤: 5ï¼‰"
    )
    parser.add_argument(
        "--rules-only",
        action="store_true",
        help="ä»…åŠ è½½å†³ç­–è§„åˆ™ï¼ˆæ›´å¿«ï¼‰"
    )
    parser.add_argument(
        "--investor", "-inv",
        help="æŒ‰æŠ•èµ„è€… ID è¿‡æ»¤ (ä¾‹å¦‚: warren_buffett)"
    )
    parser.add_argument(
        "--source-type", "-t",
        choices=["investor_doc", "rule"],
        help="æŒ‰æ¥æºç±»å‹è¿‡æ»¤"
    )
    parser.add_argument(
        "--kind", "-knd",
        choices=["entry", "exit", "risk_management", "other"],
        help="æŒ‰è§„åˆ™ç±»å‹è¿‡æ»¤ (ä»…å¯¹ rule ç±»å‹æœ‰æ•ˆ)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=900,
        help="æŠ•èµ„è€…æ–‡æ¡£åˆ†å—å¤§å° (é»˜è®¤: 900)"
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=200,
        help="åˆ†å—é‡å å¤§å° (é»˜è®¤: 200)"
    )
    parser.add_argument(
        "--format",
        choices=["text", "json"],
        default="text",
        help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: text)"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    check_dependencies()

    # æ„å»ºè¿‡æ»¤å™¨
    filter_dict = {}
    if args.investor:
        filter_dict["investor_id"] = args.investor
    if args.source_type:
        filter_dict["source_type"] = args.source_type
    if args.kind:
        filter_dict["kind"] = args.kind
    
    if not filter_dict:
        filter_dict = None

    # åŠ è½½æˆ–åˆ›å»ºå‘é‡å­˜å‚¨
    if args.load:
        load_dir = Path(args.load)
        if not load_dir.exists():
            print(f"å‘é‡åº“ç›®å½•ä¸å­˜åœ¨: {load_dir}")
            print("æç¤ºï¼šé¦–æ¬¡è¿è¡Œè¯·ä½¿ç”¨ --persist ./vectorstore å…ˆåˆ›å»ºå‘é‡åº“")
            sys.exit(1)

        print(f"åŠ è½½å‘é‡å­˜å‚¨: {load_dir}")
        vectorstore = load_vectorstore(str(load_dir))
        print("å‘é‡å­˜å‚¨åŠ è½½å®Œæˆ!")
    else:
        # åŠ è½½æ–‡æ¡£
        print("åŠ è½½æ–‡æ¡£...")

        if args.rules_only:
            documents = load_decision_rules()
            print(f"å·²åŠ è½½ {len(documents)} æ¡å†³ç­–è§„åˆ™")
        else:
            investor_docs = load_investor_documents()
            investor_docs = split_investor_documents(
                investor_docs, 
                chunk_size=args.chunk_size, 
                chunk_overlap=args.chunk_overlap
            )
            rule_docs = load_decision_rules()
            documents = investor_docs + rule_docs
            print(f"å·²åŠ è½½ {len(investor_docs)} ä¸ªæŠ•èµ„è€…æ–‡æ¡£åˆ†å— + {len(rule_docs)} æ¡å†³ç­–è§„åˆ™")

        # åˆ›å»ºå‘é‡å­˜å‚¨
        vectorstore = create_vectorstore(documents, args.persist)
        print("å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ!")
    
    # æ‰§è¡ŒæŸ¥è¯¢
    if args.interactive:
        interactive_mode(vectorstore, filter_dict=filter_dict)
    elif args.query:
        results = query_vectorstore(vectorstore, args.query, args.top_k, filter_dict=filter_dict)
        
        if args.format == "json":
            import json
            json_results = []
            for doc, score in results:
                res = doc.metadata.copy()
                res["content"] = doc.page_content
                res["similarity_estimate"] = round(1 - score, 4)
                json_results.append(res)
            print(json.dumps(json_results, ensure_ascii=False, indent=2))
        else:
            print(format_results(results))
    else:
        parser.print_help()


# === ç®€åŒ–ç‰ˆæœ¬ï¼ˆæ— éœ€å®‰è£…é¢å¤–ä¾èµ–ï¼‰===

def simple_keyword_search(query: str):
    """
    ç®€åŒ–ç‰ˆå…³é”®è¯æœç´¢ï¼ˆæ— éœ€å®‰è£… LangChainï¼‰
    
    Usage:
        from rag_langchain import simple_keyword_search
        results = simple_keyword_search("æŠ¤åŸæ²³")
    """
    import json
    import re
    
    # åŠ è½½è§„åˆ™
    rules_file = PROJECT_ROOT / "config" / "decision_rules.generated.json"
    with open(rules_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    results = []
    query_lower = query.lower()
    
    for rule in data.get("rules", []):
        when = rule.get("when", "").lower()
        then = rule.get("then", "").lower()
        because = (rule.get("because") or "").lower()
        
        # ç®€å•å…³é”®è¯åŒ¹é…
        if query_lower in when or query_lower in then or query_lower in because:
            results.append(rule)
    
    return results


if __name__ == "__main__":
    main()


