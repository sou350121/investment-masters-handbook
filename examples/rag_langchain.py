#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Investment Masters RAG Integration Example

ä½¿ç”¨ LangChain + ChromaDB å®ç°æŠ•èµ„å¤§å¸ˆçŸ¥è¯†åº“çš„ RAG æ£€ç´¢ã€‚

Requirements:
    pip install langchain langchain-community chromadb pyyaml sentence-transformers

Usage:
    python rag_langchain.py "è¿™ä¸ªè‚¡ç¥¨å€¼å¾—ä¹°å—ï¼Ÿ"
    python rag_langchain.py --interactive
    python rag_langchain.py --persist ./vectorstore "æŠ¤åŸæ²³åˆ†æ"
    python rag_langchain.py --load ./vectorstore "å·´è²ç‰¹å¦‚ä½•é€‰è‚¡ï¼Ÿ"
"""

import argparse
import os
import sys
from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    missing = []
    
    try:
        import yaml
    except ImportError:
        missing.append("pyyaml")
    
    try:
        from langchain.schema import Document
    except ImportError:
        missing.append("langchain")
    
    try:
        import chromadb
    except ImportError:
        missing.append("chromadb")

    # HuggingFaceEmbeddings é»˜è®¤ä¾èµ– sentence-transformers
    try:
        import sentence_transformers  # noqa: F401
    except ImportError:
        missing.append("sentence-transformers")
    
    if missing:
        print("ç¼ºå°‘ä¾èµ–ï¼Œè¯·å®‰è£…:")
        print(f"  pip install {' '.join(missing)}")
        sys.exit(1)


def load_investor_documents():
    """åŠ è½½æŠ•èµ„è€…æ–‡æ¡£ä¸º LangChain Document æ ¼å¼"""
    from langchain.schema import Document
    import yaml
    
    documents = []
    investors_dir = PROJECT_ROOT / "investors"
    
    # åŠ è½½æŠ•èµ„è€…ç´¢å¼•è·å–å…ƒæ•°æ®
    index_file = PROJECT_ROOT / "config" / "investor_index.yaml"
    investor_meta = {}
    
    if index_file.exists():
        with open(index_file, "r", encoding="utf-8") as f:
            index_data = yaml.safe_load(f)
            for inv in index_data.get("investors", []):
                investor_meta[inv["id"]] = inv
    
    # åŠ è½½æ¯ä¸ªæŠ•èµ„è€…çš„ Markdown æ–‡ä»¶
    for md_file in investors_dir.glob("*.md"):
        investor_id = md_file.stem
        
        with open(md_file, "r", encoding="utf-8") as f:
            content = f.read()
        
        # è·å–å…ƒæ•°æ®
        meta = investor_meta.get(investor_id, {})
        
        doc = Document(
            page_content=content,
            metadata={
                "source": str(md_file.relative_to(PROJECT_ROOT)),
                "investor_id": investor_id,
                "investor_name": meta.get("full_name", investor_id),
                "chinese_name": meta.get("chinese_name", ""),
                "style": ", ".join(meta.get("style", [])),
                "best_for": ", ".join(meta.get("best_for", [])),
            }
        )
        documents.append(doc)
    
    return documents


def split_investor_documents(documents, chunk_size: int = 900, chunk_overlap: int = 200):
    """
    å°†æŠ•èµ„è€…é•¿æ–‡æ¡£åˆ†å—ï¼Œæå‡æ£€ç´¢ç²¾åº¦ï¼Œå¹¶ä¸ºæ¯ä¸ªå—é™„åŠ å¼•ç”¨ä¿¡æ¯ã€‚

    - ä¿ç•™åŸ metadataï¼ˆsource/investor_id ç­‰ï¼‰
    - å¢åŠ  chunk_index/chunk_id/title_hint/source_type
    """
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    import re

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n## ", "\n### ", "\n#### ", "\n\n", "\n", " "],
    )

    split_docs = []

    for parent in documents:
        investor_id = parent.metadata.get("investor_id", "unknown")
        chunks = splitter.split_documents([parent])

        for idx, doc in enumerate(chunks):
            # æ ‡é¢˜æç¤ºï¼šå– chunk å†…ç¬¬ä¸€ä¸ª markdown æ ‡é¢˜
            m = re.search(r"(?m)^(#{1,4})\s+(.+?)\s*$", doc.page_content)
            title_hint = m.group(2) if m else ""

            doc.metadata["source_type"] = "investor_doc"
            doc.metadata["chunk_index"] = idx
            doc.metadata["chunk_id"] = f"{investor_id}#{idx}"
            if title_hint:
                doc.metadata["title_hint"] = title_hint

        split_docs.extend(chunks)

    return split_docs


def load_decision_rules():
    """åŠ è½½å†³ç­–è§„åˆ™ä¸º Document æ ¼å¼"""
    from langchain.schema import Document
    import json
    
    rules_file = PROJECT_ROOT / "config" / "decision_rules.generated.json"
    
    if not rules_file.exists():
        print(f"è§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨: {rules_file}")
        return []
    
    with open(rules_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    documents = []
    for rule in data.get("rules", []):
        # å°†è§„åˆ™è½¬ä¸ºè‡ªç„¶è¯­è¨€
        content = f"""
æŠ•èµ„è€…: {rule.get('investor_id', 'unknown')}
è§„åˆ™ç±»å‹: {rule.get('kind', 'other')}

IF {rule.get('when', 'N/A')}
THEN {rule.get('then', 'N/A')}
BECAUSE {rule.get('because', 'N/A')}
        """.strip()
        
        doc = Document(
            page_content=content,
            metadata={
                "source": "decision_rules.generated.json",
                "investor_id": rule.get("investor_id", "unknown"),
                "rule_id": rule.get("rule_id", ""),
                "kind": rule.get("kind", "other"),
                "source_type": "rule",
            }
        )
        documents.append(doc)
    
    return documents


def create_vectorstore(documents, persist_dir=None):
    """åˆ›å»ºå‘é‡å­˜å‚¨"""
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import Chroma
    
    # ä½¿ç”¨å…è´¹çš„æœ¬åœ° embedding æ¨¡å‹
    # å¦‚æœæœ‰ OpenAI API keyï¼Œå¯ä»¥æ”¹ç”¨ OpenAIEmbeddings
    print("åŠ è½½ embedding æ¨¡å‹...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"}
    )
    
    print(f"åˆ›å»ºå‘é‡å­˜å‚¨ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£...")
    
    if persist_dir:
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=persist_dir
        )
        vectorstore.persist()
    else:
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings
        )
    
    return vectorstore


def load_vectorstore(persist_dir: str):
    """ä»æŒä¹…åŒ–ç›®å½•åŠ è½½å‘é‡å­˜å‚¨ï¼ˆéœ€ä¸åˆ›å»ºæ—¶ä½¿ç”¨åŒä¸€ embedding é…ç½®ï¼‰"""
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import Chroma

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
    )

    return Chroma(
        persist_directory=persist_dir,
        embedding_function=embeddings,
    )


def query_vectorstore(vectorstore, query: str, k: int = 5):
    """æŸ¥è¯¢å‘é‡å­˜å‚¨"""
    results = vectorstore.similarity_search_with_score(query, k=k)
    return results


def format_results(results):
    """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
    output = []
    
    for i, (doc, score) in enumerate(results, 1):
        investor_id = doc.metadata.get("investor_id", "unknown")
        investor_name = doc.metadata.get("chinese_name") or doc.metadata.get("investor_name") or investor_id
        source = doc.metadata.get("source", "unknown")
        source_type = doc.metadata.get("source_type", "unknown")
        rule_id = doc.metadata.get("rule_id", "")
        chunk_id = doc.metadata.get("chunk_id", "")
        title_hint = doc.metadata.get("title_hint", "")

        # å¼•ç”¨ï¼šä¼˜å…ˆ rule_idï¼Œå…¶æ¬¡ chunk_id
        citation = rule_id or chunk_id or "N/A"
        
        output.append(f"\n{'='*60}")
        output.append(f"[{i}] ç›¸ä¼¼åº¦(ä¼°ç®—): {1-score:.2%} | ç±»å‹: {source_type} | æ¥æº: {source}")
        output.append(f"    æŠ•èµ„è€…: {investor_name} ({investor_id})")
        if title_hint:
            output.append(f"    ç« èŠ‚: {title_hint}")
        output.append(f"    å¼•ç”¨: {citation}")
        output.append("-" * 60)
        
        # æˆªå–å†…å®¹é¢„è§ˆ
        content = doc.page_content[:500]
        if len(doc.page_content) > 500:
            content += "..."
        output.append(content)
        output.append(f"\nğŸ“Œ å¯æº¯æºå¼•ç”¨: {source}  ->  {citation}")
    
    return "\n".join(output)


def interactive_mode(vectorstore):
    """äº¤äº’æ¨¡å¼"""
    print("\n" + "=" * 60)
    print("æŠ•èµ„å¤§å¸ˆçŸ¥è¯†åº“ - äº¤äº’æŸ¥è¯¢æ¨¡å¼")
    print("è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("=" * 60)
    
    while True:
        try:
            query = input("\nğŸ” ä½ çš„é—®é¢˜: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nå†è§!")
            break
        
        if query.lower() in ["quit", "exit", "q"]:
            print("å†è§!")
            break
        
        if not query:
            continue
        
        results = query_vectorstore(vectorstore, query)
        print(format_results(results))


def main():
    parser = argparse.ArgumentParser(
        description="Investment Masters RAG Query",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s "è¿™ä¸ªè‚¡ç¥¨å€¼å¾—ä¹°å—ï¼Ÿ"
  %(prog)s "å¸‚åœºææ…Œæ—¶è¯¥æ€ä¹ˆåŠï¼Ÿ"
  %(prog)s --interactive
  %(prog)s --persist ./vectorstore "æŠ¤åŸæ²³åˆ†æ"
  %(prog)s --load ./vectorstore "èŠ’æ ¼çš„å†³ç­–æ¸…å•"
        """
    )
    
    parser.add_argument(
        "query",
        nargs="?",
        help="æŸ¥è¯¢é—®é¢˜"
    )
    parser.add_argument(
        "--interactive", "-i",
        action="store_true",
        help="äº¤äº’æ¨¡å¼"
    )
    parser.add_argument(
        "--persist", "-p",
        help="å‘é‡å­˜å‚¨æŒä¹…åŒ–ç›®å½•"
    )
    parser.add_argument(
        "--load", "-l",
        help="åŠ è½½å·²ä¿å­˜çš„å‘é‡å­˜å‚¨ç›®å½•ï¼ˆæ›´å¿«ï¼‰"
    )
    parser.add_argument(
        "--top-k", "-k",
        type=int,
        default=5,
        help="è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤: 5ï¼‰"
    )
    parser.add_argument(
        "--rules-only",
        action="store_true",
        help="ä»…åŠ è½½å†³ç­–è§„åˆ™ï¼ˆæ›´å¿«ï¼‰"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    check_dependencies()

    # åŠ è½½æˆ–åˆ›å»ºå‘é‡å­˜å‚¨
    if args.load:
        load_dir = Path(args.load)
        if not load_dir.exists():
            print(f"å‘é‡åº“ç›®å½•ä¸å­˜åœ¨: {load_dir}")
            print("æç¤ºï¼šé¦–æ¬¡è¿è¡Œè¯·ä½¿ç”¨ --persist ./vectorstore å…ˆåˆ›å»ºå‘é‡åº“")
            sys.exit(1)

        print(f"åŠ è½½å‘é‡å­˜å‚¨: {load_dir}")
        vectorstore = load_vectorstore(str(load_dir))
        print("å‘é‡å­˜å‚¨åŠ è½½å®Œæˆ!")
    else:
        # åŠ è½½æ–‡æ¡£
        print("åŠ è½½æ–‡æ¡£...")

        if args.rules_only:
            documents = load_decision_rules()
            print(f"å·²åŠ è½½ {len(documents)} æ¡å†³ç­–è§„åˆ™")
        else:
            investor_docs = load_investor_documents()
            investor_docs = split_investor_documents(investor_docs)
            rule_docs = load_decision_rules()
            documents = investor_docs + rule_docs
            print(f"å·²åŠ è½½ {len(investor_docs)} ä¸ªæŠ•èµ„è€…æ–‡æ¡£åˆ†å— + {len(rule_docs)} æ¡å†³ç­–è§„åˆ™")

        # åˆ›å»ºå‘é‡å­˜å‚¨
        vectorstore = create_vectorstore(documents, args.persist)
        print("å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ!")
    
    # æ‰§è¡ŒæŸ¥è¯¢
    if args.interactive:
        interactive_mode(vectorstore)
    elif args.query:
        results = query_vectorstore(vectorstore, args.query, args.top_k)
        print(format_results(results))
    else:
        parser.print_help()


# === ç®€åŒ–ç‰ˆæœ¬ï¼ˆæ— éœ€å®‰è£…é¢å¤–ä¾èµ–ï¼‰===

def simple_keyword_search(query: str):
    """
    ç®€åŒ–ç‰ˆå…³é”®è¯æœç´¢ï¼ˆæ— éœ€å®‰è£… LangChainï¼‰
    
    Usage:
        from rag_langchain import simple_keyword_search
        results = simple_keyword_search("æŠ¤åŸæ²³")
    """
    import json
    import re
    
    # åŠ è½½è§„åˆ™
    rules_file = PROJECT_ROOT / "config" / "decision_rules.generated.json"
    with open(rules_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    results = []
    query_lower = query.lower()
    
    for rule in data.get("rules", []):
        when = rule.get("when", "").lower()
        then = rule.get("then", "").lower()
        because = (rule.get("because") or "").lower()
        
        # ç®€å•å…³é”®è¯åŒ¹é…
        if query_lower in when or query_lower in then or query_lower in because:
            results.append(rule)
    
    return results


if __name__ == "__main__":
    main()


