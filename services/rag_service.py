from fastapi import FastAPI, HTTPException, Header
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import os
import sys
import re
import json
import hashlib
import asyncio
import time
import traceback
from pathlib import Path
import yaml
import csv
import ast

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

from tools.rag_core import (
    load_investor_documents,
    split_investor_documents,
    load_decision_rules,
    load_vectorstore,
    create_vectorstore,
    query_vectorstore,
    ensemble_reasoning,
    build_committee_prompt,
    run_ensemble_committee,
    TieredEnsembleResponse,
)

from tools.llm_bridge import LLMBridge, LLMBridgeError, extract_json_block
from tools.reasoning_core import get_master_personality, EnsembleAdjudicator, ExpertOpinion as AdjudicatorOpinion
from services.feedback_system import FeedbackCollector, FeedbackAnalyzer

app = FastAPI(title="Investment Masters RAG API")

# åé¥‹ç³»çµ±å…¨å±€å¯¦ä¾‹
_feedback_collector: Optional[FeedbackCollector] = None
_feedback_analyzer: Optional[FeedbackAnalyzer] = None


def get_feedback_collector() -> FeedbackCollector:
    """ç²å–åé¥‹æ”¶é›†å™¨å–®ä¾‹"""
    global _feedback_collector
    if _feedback_collector is None:
        _feedback_collector = FeedbackCollector()
    return _feedback_collector


def get_feedback_analyzer() -> FeedbackAnalyzer:
    """ç²å–åé¥‹åˆ†æå™¨å–®ä¾‹"""
    global _feedback_analyzer
    if _feedback_analyzer is None:
        _feedback_analyzer = FeedbackAnalyzer(get_feedback_collector())
    return _feedback_analyzer

# å…¨å±€å‘é‡åº“å®ä¾‹
vectorstore = None
vectorstore_init_task: Optional[asyncio.Task] = None
VECTORSTORE_STATUS: Dict[str, Any] = {
    "state": "idle",  # idle | loading | ready | failed
    "attempts": 0,
    "last_attempt_ts": None,
    "last_success_ts": None,
    "last_error": None,
    "next_retry_in_s": None,
}
PERSIST_DIR = str(PROJECT_ROOT / "vectorstore")
WEB_OUT_DIR = PROJECT_ROOT / "web" / "out"
INDEX_PATH = PROJECT_ROOT / "config" / "investor_index.yaml"
POLICY_PATH = PROJECT_ROOT / "config" / "policy_gate.yaml"
SCENARIOS_PATH = PROJECT_ROOT / "config" / "scenarios.yaml"
AUDIT_DIR = PROJECT_ROOT / "logs"
AUDIT_PATH = AUDIT_DIR / "policy_gate_audit.jsonl"
DEFAULT_BACKTEST_RESULTS_ROOT = "results"

_index_cache: Optional[Dict[str, Any]] = None


def _require_bearer_token(authorization: Optional[str]) -> str:
    """
    nofx-style API protection:
    - Expect: Authorization: Bearer <token>
    - Token source: IMH_API_TOKEN (env) OR the token itself is an LLM Key (starts with sk-)
    Returns the token string if valid.
    """
    auth = (authorization or "").strip()
    if not auth:
        raise HTTPException(status_code=401, detail="Unauthorized: Authorization header missing")

    parts = auth.split(" ", 1)
    if len(parts) != 2 or parts[0] != "Bearer":
        raise HTTPException(status_code=401, detail="Invalid Authorization format (Expected Bearer <token>)")

    token = parts[1].strip()
    
    # 1. Check if it's the specific access token for this IMH instance
    expected = (os.getenv("IMH_API_TOKEN") or "").strip()
    if expected and token == expected:
        return token

    # 2. Check if it looks like an LLM API Key (nofx mode)
    # Most keys start with 'sk-' (OpenAI, OpenRouter, Anthropic, etc.)
    if token.startswith("sk-") or token.startswith("or-"):
        return token

    # 3. If IMH_API_TOKEN is not set, we might be in "open access" mode or "only LLM key" mode
    if not expected:
        # If no expected token, we only allow LLM keys
        raise HTTPException(status_code=401, detail="Unauthorized: IMH_API_TOKEN not set and token is not an LLM key")

    raise HTTPException(status_code=401, detail="Unauthorized: Invalid token")


def _get_vectorstore_doc_count(vs: Any) -> Optional[int]:
    """
    Best-effort: Chroma exposes _collection.count(). Keep it defensive.
    """
    if vs is None:
        return None
    try:
        col = getattr(vs, "_collection", None)
        if col is not None and hasattr(col, "count"):
            return int(col.count())
    except Exception:
        return None
    return None


def _maybe_require_token(authorization: Optional[str]) -> Optional[str]:
    """
    Optional auth: if IMH_API_TOKEN is set (instance protected) OR client sends Authorization,
    we enforce Bearer token checks. Otherwise allow open access (local dev friendly).
    """
    if os.getenv("IMH_API_TOKEN") or (authorization and "Bearer" in str(authorization)):
        return _require_bearer_token(authorization)
    return None


def _safe_results_root(root: Optional[str]) -> Path:
    """
    Resolve a results root directory under PROJECT_ROOT, preventing path traversal.
    Allows roots like: results, results_sim_sharpe, etc (must live directly under PROJECT_ROOT).
    """
    r = (root or DEFAULT_BACKTEST_RESULTS_ROOT).strip().strip("/\\")
    if not r:
        r = DEFAULT_BACKTEST_RESULTS_ROOT
    if any(x in r for x in ("..", "/", "\\")):
        raise HTTPException(status_code=400, detail="Invalid results root")
    p = (PROJECT_ROOT / r).resolve()
    pr = PROJECT_ROOT.resolve()
    if pr not in p.parents and p != pr:
        raise HTTPException(status_code=400, detail="Invalid results root")
    return p


def _read_json_file(path: Path) -> Optional[Dict[str, Any]]:
    try:
        if not path.exists():
            return None
        data = json.loads(path.read_text(encoding="utf-8") or "{}")
        return data if isinstance(data, dict) else None
    except Exception:
        return None


def _read_text_file(path: Path, max_chars: int = 200_000) -> Optional[str]:
    try:
        if not path.exists():
            return None
        s = path.read_text(encoding="utf-8", errors="replace")
        if len(s) > max_chars:
            return s[:max_chars] + "\n\n...[truncated]..."
        return s
    except Exception:
        return None


def _read_equity_curve_csv(path: Path, max_points: int = 800) -> List[Dict[str, Any]]:
    """
    Read Series-like CSV (date,index -> value) produced by pandas Series.to_csv().
    Returns downsampled points: [{date, equity}].
    """
    if not path.exists():
        return []
    pts: List[Dict[str, Any]] = []
    try:
        with path.open("r", encoding="utf-8", newline="") as f:
            reader = csv.reader(f)
            _header = next(reader, None)
            for row in reader:
                if not row or len(row) < 2:
                    continue
                date_s = str(row[0]).strip()
                if not date_s:
                    continue
                try:
                    v = float(row[1])
                except Exception:
                    continue
                pts.append({"date": date_s, "equity": v})
    except Exception:
        return []

    if max_points > 0 and len(pts) > max_points:
        # Evenly sample and keep the last point for correct end-of-period display.
        step = max(1, int((len(pts) + max_points - 1) / max_points))
        ds = pts[::step]
        if ds and pts and ds[-1].get("date") != pts[-1].get("date"):
            ds.append(pts[-1])
        pts = ds

    return pts


def _read_history_csv(path: Path, max_rows: int = 500) -> List[Dict[str, Any]]:
    """
    Read history_A.csv / history_B.csv and normalize allocation fields to dict.
    """
    if not path.exists():
        return []
    out: List[Dict[str, Any]] = []
    try:
        with path.open("r", encoding="utf-8", newline="") as f:
            reader = csv.DictReader(f)
            for i, row in enumerate(reader):
                if max_rows > 0 and i >= max_rows:
                    break
                r = dict(row or {})
                # allocation is stored as python dict string in CSV by pandas
                alloc_raw = r.get("allocation")
                alloc: Optional[Dict[str, Any]] = None
                if isinstance(alloc_raw, str) and alloc_raw.strip():
                    s = alloc_raw.strip()
                    try:
                        alloc = json.loads(s)
                    except Exception:
                        try:
                            v = ast.literal_eval(s)
                            alloc = v if isinstance(v, dict) else None
                        except Exception:
                            alloc = None
                if alloc is not None:
                    r["allocation"] = alloc
                # equity numeric
                if "equity" in r:
                    try:
                        r["equity"] = float(r["equity"])
                    except Exception:
                        pass
                out.append(r)
    except Exception:
        return []
    return out


class BacktestRunSummary(BaseModel):
    run_id: str
    root: str
    last_modified_ts: float
    last_modified_iso: str
    modes: List[str] = []
    metrics: Dict[str, Any] = {}


class BacktestRunDetail(BaseModel):
    run_id: str
    root: str
    files: Dict[str, bool] = {}
    config: Dict[str, Any] = {}
    metrics: Dict[str, Any] = {}
    equity: Dict[str, Any] = {}
    history: Dict[str, Any] = {}
    comparison_md: Optional[str] = None


@app.get("/api/backtest/runs", response_model=Dict[str, Any])
async def list_backtest_runs(root: Optional[str] = None, authorization: Optional[str] = Header(None)):
    _maybe_require_token(authorization)
    root_dir = _safe_results_root(root)
    if not root_dir.exists():
        return {"root": str(root_dir.name), "runs": []}

    runs: List[BacktestRunSummary] = []
    try:
        for child in root_dir.iterdir():
            if not child.is_dir():
                continue
            run_id = child.name
            try:
                st = child.stat()
                mtime = float(st.st_mtime)
            except Exception:
                mtime = 0.0

            metrics_a = _read_json_file(child / "metrics_A.json")
            metrics_b = _read_json_file(child / "metrics_B.json")
            modes = []
            metrics: Dict[str, Any] = {}
            if metrics_a is not None:
                modes.append("A")
                metrics["A"] = metrics_a
            if metrics_b is not None:
                modes.append("B")
                metrics["B"] = metrics_b

            # Only list folders that look like a backtest run (has any known file)
            has_any = any(
                (child / fn).exists()
                for fn in (
                    "metrics_A.json",
                    "metrics_B.json",
                    "equity_curve_A.csv",
                    "equity_curve_B.csv",
                    "history_A.csv",
                    "history_B.csv",
                    "comparison.md",
                )
            )
            if not has_any:
                continue

            iso = time.strftime("%Y-%m-%dT%H:%M:%S", time.localtime(mtime))
            runs.append(
                BacktestRunSummary(
                    run_id=run_id,
                    root=str(root_dir.name),
                    last_modified_ts=mtime,
                    last_modified_iso=iso,
                    modes=modes,
                    metrics=metrics,
                )
            )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list backtest runs: {e}")

    runs.sort(key=lambda r: r.last_modified_ts, reverse=True)
    return {"root": str(root_dir.name), "runs": [r.model_dump() for r in runs]}


@app.get("/api/backtest/runs/{run_id}", response_model=BacktestRunDetail)
async def get_backtest_run(run_id: str, root: Optional[str] = None, authorization: Optional[str] = Header(None)):
    _maybe_require_token(authorization)
    root_dir = _safe_results_root(root)
    if not run_id or "/" in run_id or "\\" in run_id or ".." in run_id:
        raise HTTPException(status_code=400, detail="Invalid run_id")
    run_dir = (root_dir / run_id).resolve()
    pr = PROJECT_ROOT.resolve()
    if pr not in run_dir.parents and run_dir != pr:
        raise HTTPException(status_code=400, detail="Invalid run path")
    if not run_dir.exists() or not run_dir.is_dir():
        raise HTTPException(status_code=404, detail="Backtest run not found")

    files = {
        "metrics_A": (run_dir / "metrics_A.json").exists(),
        "metrics_B": (run_dir / "metrics_B.json").exists(),
        "equity_curve_A": (run_dir / "equity_curve_A.csv").exists(),
        "equity_curve_B": (run_dir / "equity_curve_B.csv").exists(),
        "history_A": (run_dir / "history_A.csv").exists(),
        "history_B": (run_dir / "history_B.csv").exists(),
        "comparison": (run_dir / "comparison.md").exists(),
        "run_config": (run_dir / "run_config.json").exists(),
    }

    config = _read_json_file(run_dir / "run_config.json") or {}
    metrics: Dict[str, Any] = {}
    ma = _read_json_file(run_dir / "metrics_A.json")
    mb = _read_json_file(run_dir / "metrics_B.json")
    if ma is not None:
        metrics["A"] = ma
    if mb is not None:
        metrics["B"] = mb

    equity = {}
    if files["equity_curve_A"]:
        equity["A"] = _read_equity_curve_csv(run_dir / "equity_curve_A.csv", max_points=900)
    if files["equity_curve_B"]:
        equity["B"] = _read_equity_curve_csv(run_dir / "equity_curve_B.csv", max_points=900)

    history = {}
    if files["history_A"]:
        history["A"] = _read_history_csv(run_dir / "history_A.csv", max_rows=800)
    if files["history_B"]:
        history["B"] = _read_history_csv(run_dir / "history_B.csv", max_rows=800)

    comparison_md = _read_text_file(run_dir / "comparison.md")
    return BacktestRunDetail(
        run_id=run_id,
        root=str(root_dir.name),
        files=files,
        config=config,
        metrics=metrics,
        equity=equity,
        history=history,
        comparison_md=comparison_md,
    )


def _load_index() -> Dict[str, Any]:
    global _index_cache
    if _index_cache is not None:
        return _index_cache
    if not INDEX_PATH.exists():
        _index_cache = {}
        return _index_cache
    data = yaml.safe_load(INDEX_PATH.read_text(encoding="utf-8")) or {}
    _index_cache = data
    return data


_policy_cache: Optional[Dict[str, Any]] = None
_policy_cache_hash: Optional[str] = None


def _load_policy() -> Dict[str, Any]:
    global _policy_cache, _policy_cache_hash
    if not POLICY_PATH.exists():
        _policy_cache = {}
        _policy_cache_hash = None
        return {}

    raw = POLICY_PATH.read_text(encoding="utf-8")
    h = hashlib.sha256(raw.encode("utf-8")).hexdigest()
    if _policy_cache is not None and _policy_cache_hash == h:
        return _policy_cache

    data = yaml.safe_load(raw) or {}
    _policy_cache = data
    _policy_cache_hash = h
    return data


def _safe_float(x: Any) -> Optional[float]:
    try:
        if x is None:
            return None
        return float(x)
    except Exception:
        return None


def _cmp(op: str, a: float, b: float) -> bool:
    if op == ">":
        return a > b
    if op == ">=":
        return a >= b
    if op == "<":
        return a < b
    if op == "<=":
        return a <= b
    if op == "==":
        return a == b
    return False


def _cmp_expectation(op: str, actual: float, expected: float, tol: Optional[float] = None) -> bool:
    """
    Compare actual vs expected for scenario expectations (supports "~" / approx).
    Keep deterministic and minimal; do NOT affect core Policy Gate scoring logic.
    """
    if op in ("~", "â‰ˆ", "approx"):
        t = float(tol) if tol is not None else 0.05
        return abs(actual - expected) <= t
    if op == "!=":
        return actual != expected
    return _cmp(op, actual, expected)


def _clamp(x: float, lo: float, hi: float) -> float:
    return max(lo, min(hi, x))


def _hash_input(obj: Any) -> str:
    try:
        s = json.dumps(obj, ensure_ascii=False, sort_keys=True)
    except Exception:
        s = str(obj)
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def _match_scenarios(text: str) -> List[str]:
    t = (text or "").strip()
    t_low = t.lower()

    scenario_keywords = {
        "å¸‚åœºææ…Œ": ["ææ…Œ", "æš´è·Œ", "å´©ç›˜", "è¸©è¸", "æµåŠ¨æ€§å±æœº", "æŒ¤å…‘", "panic", "crash", "selloff", "limit down", "è·Œåœ"],
        "å¸‚åœºç‹‚çƒ­": ["ç‹‚çƒ­", "fomo", "all in", "è¿½é«˜", "è¿‡çƒ­", "ç–¯æ¶¨", "é€¼ç©º", "æŒ¤ç©º", "meme", "çƒ­é’±"],
        "ç»æµè¡°é€€": ["è¡°é€€", "è§æ¡", "å¤±ä¸š", "ç»æµä¸‹è¡Œ", "recession", "hard landing", "soft landing"],
        "åˆ©ç‡è½¬å‘": ["é™æ¯", "åŠ æ¯", "åˆ©ç‡", "yield", "rates", "è½¬å‘", "æ‹ç‚¹", "ä¼šè®®çºªè¦", "ç‚¹é˜µå›¾"],
        "æµåŠ¨æ€§æ”¶ç´§": ["ç¼©è¡¨", "æ”¶ç´§", "ç´§ç¼©", "æµåŠ¨æ€§", "tga", "rrp", "qt", "qe", "èèµ„", "ä¿è¯é‡‘"],
        "ä¼°å€¼æ³¡æ²«": ["ä¼°å€¼", "æ³¡æ²«", "è¿‡é«˜", "å¸‚ç›ˆç‡", "pe", "ps", "pb", "ev/ebitda", "ä¼°å€¼æ‰©å¼ "],
        "åˆ›ä¸šä¸äº§å“åŒ–": ["åˆ›ä¸š", "åˆä¼™äºº", "äº§å“åŒ–", "è‚¡æƒ", "ç‰¹å®šçŸ¥è¯†", "ä¸å¯æ›¿ä»£", "MVP", "startup"],
        "æ æ†ä¸è´¢å¯Œ": ["æ æ†", "ä»£ç ", "åª’ä½“", "åŠ³åŠ¨åŠ›", "è´¢å¯Œç§¯ç´¯", "æ‰€æœ‰æƒ", "leverage", "equity"],
        "å¹¸ç¦å“²å­¦": ["å¹¸ç¦", "å®é™", "æ¬²æœ›", "ä¹ æƒ¯", "å†¥æƒ³", "èº«å¿ƒç³»ç»Ÿ", "happiness"],
        "æ”¿ç­–ä¸æ³•æ¡ˆ": ["æ”¿ç­–", "æ³•æ¡ˆ", "ç«‹æ³•", "è¡¥è´´", "åŠ ç¨", "æŠ«éœ²", "å¯¹å†²", "policy", "bill", "legislation", "disclosure"],
        "åœ°äº§ä¸äº¤æ˜“": ["åœ°äº§", "è°ˆåˆ¤", "ç­¹ç è‰ºæœ¯", "åœ°æ®µ", "å†èèµ„", "åœ°æ ‡", "real estate", "negotiation", "deal"],
        "è´¢å•†ä¸ç°é‡‘æµ": ["è´¢å•†", "ç°é‡‘æµ", "èµ„äº§è´Ÿå€º", "è¢«åŠ¨æ”¶å…¥", "è€é¼ èµ›è·‘", "esbi", "cash flow", "rich dad"],
        "å™äº‹ä¸é£æŠ•": ["å™äº‹", "è¶‹åŠ¿", "ç§‘æŠ€é£å£", "åèº«æ€§", "éå¯¹ç§°", "èˆ†è®ºæ æ†", "vc", "narrative", "chamath"],
        "ç¡¬èµ„äº§": ["é»„é‡‘", "ç™½é“¶", "æ¯”ç‰¹å¸", "æŠ—é€šèƒ€", "å°é’", "gold", "silver", "bitcoin", "btc", "inflation"],
    }

    matched: List[str] = []
    for scen, keys in scenario_keywords.items():
        if any(k.lower() in t_low for k in keys):
            matched.append(scen)
    return matched


def _route_investors(text: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    Very simple local-first router:
    - scenario keyword match -> consult_order boosts
    - per-investor keyword match from tags_zh / key_concepts / style / best_for / fund boosts
    Returns ranked investors with reasons.
    """
    idx = _load_index()
    investors = idx.get("investors", []) or []
    scenario_routing = idx.get("scenario_routing", {}) or {}

    t = (text or "").strip()
    t_low = t.lower()

    quick_lookup = (idx.get("quick_lookup", {}) or {}).get("by_question", {}) or {}

    # Scenario keyword heuristics (Chinese + English-ish)
    matched_scenarios = _match_scenarios(t)

    # Decision intent keywords -> investor boosts (very novice-friendly)
    intent_keywords = {
        "ä¹°å…¥/è¿½é«˜": (["ä¹°", "ä¹°å…¥", "å¼€ä»“", "è¿½", "è¿½å—", "chase", "enter"], ["warren_buffett", "peter_lynch", "charlie_munger"]),
        "å–å‡º/æ­¢ç›ˆ": (["å–", "å–å‡º", "æ­¢ç›ˆ", "take profit", "exit"], ["howard_marks", "stanley_druckenmiller", "george_soros"]),
        "æ­¢æŸ/é£æ§": (["æ­¢æŸ", "é£æ§", "å›æ’¤", "ä»“ä½", "max drawdown", "risk", "position sizing"], ["george_soros", "stanley_druckenmiller", "seth_klarman", "naval_ravikant"]),
        "å®è§‚/æ”¿ç­–": (["å®è§‚", "åˆ©ç‡", "é€šèƒ€", "å°±ä¸š", "ç¾å…ƒ", "æ”¿ç­–", "fed", "cpi", "ppi", "æ³•æ¡ˆ", "ç«‹æ³•", "å›½ä¼š"], ["ray_dalio", "stanley_druckenmiller", "george_soros", "nancy_pelosi"]),
        "å‘¨æœŸ/æƒ…ç»ª": (["å‘¨æœŸ", "æƒ…ç»ª", "æç«¯", "ææ…Œ", "ç‹‚çƒ­", "sentiment", "cycle"], ["howard_marks", "ray_dalio", "charlie_munger", "donald_trump"]),
        # Valuation (novice-friendly)
        "ä¼°å€¼/å®‰å…¨è¾¹é™…": (
            [
                "ä¼°å€¼",
                "é«˜ä¼°",
                "ä½ä¼°",
                "è´µä¸è´µ",
                "ä¾¿ä¸ä¾¿å®œ",
                "å€¼ä¸å€¼",
                "åˆç†å—",
                "æº¢ä»·",
                "æŠ˜ä»·",
                "å®‰å…¨è¾¹é™…",
                "å†…åœ¨ä»·å€¼",
                "ä»·å€¼é™·é˜±",
                "pe",
                "pb",
                "ps",
                "ev/ebitda",
                "intrinsic value",
                "undervalued",
                "overvalued",
                "cheap",
                "expensive",
            ],
            ["warren_buffett", "seth_klarman", "howard_marks", "charlie_munger"],
        ),
        "æˆé•¿/PEG": (["æˆé•¿", "peg", "è¥æ”¶å¢é•¿", "å¢é€Ÿ", "tenbagger"], ["peter_lynch", "warren_buffett"]),
        "é‡åŒ–/å› å­": (["é‡åŒ–", "å› å­", "æ¨¡å‹", "å›æµ‹", "å› å­æŠ•èµ„", "factor"], ["james_simons", "ed_thorp", "cliff_asness"]),
        "äº‹ä»¶/æ¿€è¿›": (["å¹¶è´­", "åˆ†æ‹†", "å›è´­", "é‡ç»„", "è‚¡ä¸œè¡ŒåŠ¨", "proxy fight", "activist"], ["carl_icahn", "seth_klarman"]),
        "åˆ›ä¸š/æ æ†": (["åˆ›ä¸š", "äº§å“åŒ–", "æ æ†", "ç‰¹å®šçŸ¥è¯†", "è‚¡æƒ", "æ‰€æœ‰æƒ", "å¤©ä½¿æŠ•èµ„", "è‡ªç”±", "wealth"], ["naval_ravikant", "charlie_munger"]),
        "åœ°äº§/è°ˆåˆ¤": (["åœ°äº§", "æˆ¿å­", "å†™å­—æ¥¼", "è°ˆåˆ¤", "ç­¹ç ", "å“ç‰Œæº¢ä»·", "å†èèµ„", "æ æ†ç»è¥", "real estate"], ["donald_trump"]),
        "æœŸæƒ/æŠ«éœ²": (["æœŸæƒ", "çœ‹æ¶¨", "çœ‹è·Œ", "æŠ«éœ²", "å›½ä¼šäº¤æ˜“", "å†…å¹•", "è·Ÿå•", "options", "leaps"], ["nancy_pelosi", "ed_thorp"]),
        "è´¢å•†/é€šèƒ€": (["è´¢å•†", "ç°é‡‘æµ", "ç¡¬èµ„äº§", "é€šèƒ€", "é»„é‡‘", "æ¯”ç‰¹å¸", "èµ„äº§è´Ÿå€º", "è¢«åŠ¨æ”¶å…¥"], ["robert_kiyosaki", "ray_dalio"]),
        "å™äº‹/ç§‘æŠ€": (["å™äº‹", "é£å£", "ç§‘æŠ€è¶‹åŠ¿", "åèº«æ€§", "éå¯¹ç§°", "è·¨å‘¨æœŸ"], ["chamath_palihapitiya", "naval_ravikant"]),
    }

    # Base score table
    scores: Dict[str, float] = {}
    reasons: Dict[str, List[str]] = {}

    def add(iid: str, delta: float, why: str):
        scores[iid] = scores.get(iid, 0.0) + delta
        reasons.setdefault(iid, []).append(why)

    # Quick lookup exact phrases (from YAML)
    for q, ids in quick_lookup.items():
        if q and q in t:
            for iid in (ids or []):
                add(iid, 3.5, f"åŒ¹é…å¿«é€Ÿé—®é¢˜ã€Œ{q}ã€")

    # Scenario match boosts consult_order
    for scen in matched_scenarios:
        route = scenario_routing.get(scen, {}) or {}
        consult = route.get("consult_order", []) or []
        for rank, iid in enumerate(consult):
            add(iid, 3.0 - min(rank, 3) * 0.5, f"åŒ¹é…æƒ…å¢ƒã€Œ{scen}ã€")

    # Intent boosts (buy/sell/stoploss/macro etc.)
    for intent, (keys, ids) in intent_keywords.items():
        if any(k.lower() in t_low for k in keys):
            for iid in ids:
                add(iid, 1.8, f"åŒ¹é…æ„å›¾ã€Œ{intent}ã€")

    # Explicit over/under valuation routing (stronger signal)
    underval_keys = ["ä½ä¼°", "ä¾¿å®œ", "æŠ˜ä»·", "underpriced", "undervalued", "cheap"]
    overval_keys = ["é«˜ä¼°", "å¤ªè´µ", "è´µ", "æº¢ä»·", "overpriced", "overvalued", "expensive"]
    if any(k.lower() in t_low for k in underval_keys):
        for iid in ["seth_klarman", "warren_buffett", "howard_marks"]:
            add(iid, 2.6, "åˆ¤æ–­æ˜¯å¦ä½ä¼°/æœ‰å®‰å…¨è¾¹é™…")
    if any(k.lower() in t_low for k in overval_keys):
        for iid in ["charlie_munger", "michael_burry", "howard_marks", "george_soros"]:
            add(iid, 2.6, "åˆ¤æ–­æ˜¯å¦é«˜ä¼°/æ³¡æ²«ä¸é£é™©")

    # Ticker hints (AAPL/TSLA/NVDA etc.) -> treat as stock selection
    if re.search(r"\b[A-Z]{1,5}\b", text or ""):
        for iid in ["warren_buffett", "peter_lynch", "charlie_munger"]:
            add(iid, 1.2, "è¯†åˆ«åˆ°ä»£ç /è‚¡ç¥¨ç¼©å†™ï¼Œåå‘é€‰è‚¡è§†è§’")

    # Per-investor keyword match
    for inv in investors:
        iid = inv.get("id")
        if not iid:
            continue

        # Name direct match
        cn = (inv.get("chinese_name") or "").strip()
        en = (inv.get("full_name") or "").strip()
        if cn and cn in t:
            add(iid, 5.0, "æ–‡æœ¬ä¸­ç›´æ¥æåˆ°è¯¥å¤§å¸ˆå§“å")
        if en and en.lower() in t_low:
            add(iid, 5.0, "æ–‡æœ¬ä¸­ç›´æ¥æåˆ°è¯¥å¤§å¸ˆè‹±æ–‡å")

        # Field matchers
        fields: List[str] = []
        for k in ("tags_zh", "style", "best_for"):
            vals = inv.get(k) or []
            if isinstance(vals, list):
                fields.extend([str(x) for x in vals if x])

        # key_concepts items may contain "moat (æŠ¤åŸæ²³)" -> both tokens
        key_concepts = inv.get("key_concepts") or []
        if isinstance(key_concepts, list):
            for x in key_concepts:
                s = str(x)
                fields.append(s)
                m = re.search(r"\(([^)]+)\)", s)
                if m:
                    fields.append(m.group(1))

        # fund/company
        fund = inv.get("fund")
        if fund:
            fields.append(str(fund))

        # score if any field token appears in text
        matched = []
        for token in set(fields):
            if not token:
                continue
            token_low = token.lower()
            if (len(token) >= 2 and token in t) or (len(token_low) >= 3 and token_low in t_low):
                matched.append(token)
        if matched:
            add(iid, min(3.0, 0.6 * len(matched)), f"åŒ¹é…å…³é”®è¯ï¼š{', '.join(matched[:6])}" + ("â€¦" if len(matched) > 6 else ""))

    # Build output
    by_id = {inv.get("id"): inv for inv in investors if inv.get("id")}
    ranked = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)
    out: List[Dict[str, Any]] = []
    for iid, sc in ranked[: max(1, min(int(top_k or 5), 10))]:
        inv = by_id.get(iid) or {}
        out.append(
            {
                "investor_id": iid,
                "chinese_name": inv.get("chinese_name") or iid,
                "full_name": inv.get("full_name") or iid,
                "nationality": inv.get("nationality"),
                "fund": inv.get("fund"),
                "intro_zh": inv.get("intro_zh"),
                "score": round(sc, 3),
                "reasons": reasons.get(iid, [])[:5],
                "matched_scenarios": matched_scenarios,
            }
        )

    # Ensure we always return enough candidates for novices (fill with sensible defaults).
    target_n = max(1, min(int(top_k or 5), 10))
    defaults = ["warren_buffett", "charlie_munger", "howard_marks", "ray_dalio", "seth_klarman"]
    existing = {x.get("investor_id") for x in out}
    if len(out) < target_n:
        for iid in defaults:
            if len(out) >= target_n:
                break
            if iid in existing:
                continue
            inv = by_id.get(iid) or {}
            out.append(
                {
                    "investor_id": iid,
                    "chinese_name": inv.get("chinese_name") or iid,
                    "full_name": inv.get("full_name") or iid,
                    "nationality": inv.get("nationality"),
                    "fund": inv.get("fund"),
                    "intro_zh": inv.get("intro_zh"),
                    "score": 0.0,
                    "reasons": ["ä¿¡æ¯ä¸è¶³/å…³é”®è¯ä¸æ˜æ˜¾ï¼Œè¡¥é½é€šç”¨ç»„åˆï¼ˆä»·å€¼/å‘¨æœŸ/å®è§‚/é£æ§ï¼‰"],
                    "matched_scenarios": matched_scenarios,
                }
            )
    return out

class QueryRequest(BaseModel):
    query: str
    top_k: int = 5
    investor_id: Optional[str] = None
    source_type: Optional[str] = None
    kind: Optional[str] = None

class QueryResponse(BaseModel):
    content: str
    metadata: Dict[str, Any]
    similarity_estimate: float


class RouteRequest(BaseModel):
    text: str
    top_k: int = 5


class RouteResponse(BaseModel):
    investor_id: str
    chinese_name: str
    full_name: str
    nationality: Optional[str] = None
    fund: Optional[str] = None
    intro_zh: Optional[str] = None
    score: float
    reasons: List[str]
    matched_scenarios: List[str] = []


class SaveScenariosRequest(BaseModel):
    scenarios: List[Dict[str, Any]]


class ValidationItem(BaseModel):
    scenario_id: str
    label: str
    passed: bool
    details: List[str]
    results: Optional[Dict[str, Any]] = None


class ValidationReport(BaseModel):
    total: int
    passed_count: int
    failed_count: int
    items: List[ValidationItem]


# ---------------- Policy Gate (Fund Stack) ----------------
class PolicyGateRequest(BaseModel):
    # Natural language market observations / thesis / news summary
    text: str
    # Structured features (VIX, rates, spreads, inflation, breadth, etc.)
    features: Dict[str, float] = {}
    # Portfolio live state (leverage, cash, drawdown, turnover, corr, concentration)
    portfolio_state: Dict[str, float] = {}
    # Hard constraints / limits (optional)
    constraints: Dict[str, float] = {}
    top_k_router: int = 5
    top_k_rule_hits: int = 8


class RiskOverlay(BaseModel):
    multipliers: Dict[str, float]
    absolute: Dict[str, float]


class EvidenceItem(BaseModel):
    content: str
    metadata: Dict[str, Any]
    similarity_estimate: float


class PolicyGateResponse(BaseModel):
    regime: Dict[str, Any]
    scenario: Dict[str, Any]
    router: List[RouteResponse]
    rule_hits: List[EvidenceItem]
    risk_overlay: RiskOverlay
    explanation: Dict[str, Any]
    audit: Dict[str, Any]

class EnsembleRequest(BaseModel):
    query: str
    top_n_rules: int = 20
    top_k_experts: int = 3


@app.post("/api/rag/ensemble", response_model=TieredEnsembleResponse)
async def rag_ensemble(req: EnsembleRequest, authorization: Optional[str] = Header(None)):
    token = _require_bearer_token(authorization)
    if not req.query or not req.query.strip():
        raise HTTPException(status_code=400, detail="query is required")
    if vectorstore is None:
        raise HTTPException(status_code=503, detail="Vectorstore not ready")

    # Step 1: Initialize bridge with potential token override
    bridge = LLMBridge()
    if token.startswith("sk-") or token.startswith("or-"):
        bridge.set_api_key(token)

    try:
        # Step 2: call shared ensemble committee logic
        # We wrap this in to_thread because it involves synchronous network calls (LLM) 
        # and vectorstore queries.
        from tools.rag_core import run_ensemble_committee
        result = await asyncio.to_thread(
            run_ensemble_committee,
            vectorstore,
            req.query.strip(),
            bridge,
            req.top_n_rules,
            req.top_k_experts
        )
        return TieredEnsembleResponse(**result)
        
    except LLMBridgeError as e:
        raise HTTPException(status_code=503, detail=str(e))
    except ValueError as e:
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        print(f"Ensemble error: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"ensemble llm error: {e}")
@app.on_event("startup")
async def startup_event():
    global vectorstore, vectorstore_init_task
    # IMPORTANT: do not block app startup on vectorstore init.
    # We start the server quickly (so the web UI is visible), then init vectorstore in background.
    print(f"æ­£åœ¨åˆå§‹åŒ– RAG æœåŠ¡ï¼ˆåå°åŠ è½½å‘é‡åº“ï¼‰ï¼ŒæŒä¹…åŒ–ç›®å½•: {PERSIST_DIR}")

    async def _init_vectorstore_bg():
        global vectorstore, VECTORSTORE_STATUS

        def _sync_init():
            vs = None
            if os.path.exists(PERSIST_DIR):
                print("å‘ç°å·²æŒä¹…åŒ–çš„å‘é‡åº“ï¼Œæ­£åœ¨åŠ è½½...")
                try:
                    vs = load_vectorstore(PERSIST_DIR)
                    print("å‘é‡åº“åŠ è½½æˆåŠŸ!")
                    return vs
                except Exception as e:
                    print(f"åŠ è½½å¤±è´¥: {e}ï¼Œå°†é‡æ–°æ„å»º...")
                    vs = None

            print("æ­£åœ¨æ„å»ºæ–°çš„å‘é‡åº“ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
            investor_docs = load_investor_documents()
            investor_docs = split_investor_documents(investor_docs)
            rule_docs = load_decision_rules()
            all_docs = investor_docs + rule_docs
            vs = create_vectorstore(all_docs, PERSIST_DIR)
            print("å‘é‡åº“æ„å»ºå¹¶ä¿å­˜æˆåŠŸ!")
            return vs

        delay = 1.0
        while True:
            VECTORSTORE_STATUS["state"] = "loading"
            VECTORSTORE_STATUS["attempts"] = int(VECTORSTORE_STATUS.get("attempts") or 0) + 1
            VECTORSTORE_STATUS["last_attempt_ts"] = time.time()
            VECTORSTORE_STATUS["next_retry_in_s"] = None
            try:
                vectorstore = await asyncio.to_thread(_sync_init)
                VECTORSTORE_STATUS["state"] = "ready"
                VECTORSTORE_STATUS["last_success_ts"] = time.time()
                VECTORSTORE_STATUS["last_error"] = None
                VECTORSTORE_STATUS["next_retry_in_s"] = None
                print(f"å‘é‡åº“å°±ç»ªï¼šdoc_count={_get_vectorstore_doc_count(vectorstore)}")
                return
            except Exception as e:
                # keep service up, but mark vectorstore as unavailable
                vectorstore = None
                VECTORSTORE_STATUS["state"] = "failed"
                VECTORSTORE_STATUS["last_error"] = f"{type(e).__name__}: {e}"
                VECTORSTORE_STATUS["next_retry_in_s"] = delay
                print(f"å‘é‡åº“åˆå§‹åŒ–å¤±è´¥ï¼ˆå°†é‡è¯•ï¼ŒæœåŠ¡ä»å¯ç”¨ï¼‰ï¼š{type(e).__name__}: {e}")
                print(traceback.format_exc())
                await asyncio.sleep(delay)
                delay = min(delay * 2.0, 60.0)

    if vectorstore_init_task is None or vectorstore_init_task.done():
        vectorstore_init_task = asyncio.create_task(_init_vectorstore_bg())

@app.get("/health")
async def health():
    # persistent dir stats (best-effort)
    persist_exists = os.path.exists(PERSIST_DIR)
    file_count = 0
    total_bytes = 0
    if persist_exists:
        try:
            for root, _dirs, files in os.walk(PERSIST_DIR):
                for fn in files:
                    file_count += 1
                    try:
                        total_bytes += os.path.getsize(os.path.join(root, fn))
                    except Exception:
                        pass
        except Exception:
            pass

    doc_count = _get_vectorstore_doc_count(vectorstore)
    return {
        "status": "ok",
        "vectorstore_ready": vectorstore is not None,
        "vectorstore_doc_count": doc_count,
        "vectorstore_status": VECTORSTORE_STATUS,
        "persist_dir": PERSIST_DIR,
        "persist_dir_exists": persist_exists,
        "persist_dir_file_count": file_count,
        "persist_dir_total_bytes": total_bytes,
    }


# Backwards/interop alias (some clients probe /api/health)
@app.get("/api/health", include_in_schema=False)
@app.get("/api/health/", include_in_schema=False)
async def health_alias():
    return await health()


@app.get("/api/policy/scenarios")
async def get_scenarios():
    if not SCENARIOS_PATH.exists():
        return {"scenarios": []}
    try:
        data = yaml.safe_load(SCENARIOS_PATH.read_text(encoding="utf-8")) or {}
        return data
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/policy/scenarios")
async def save_scenarios(req: SaveScenariosRequest):
    try:
        data = {"scenarios": req.scenarios}
        SCENARIOS_PATH.write_text(yaml.dump(data, allow_unicode=True, sort_keys=False), encoding="utf-8")
        return {"status": "success", "count": len(req.scenarios)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/policy/validate_all", response_model=ValidationReport)
async def validate_all_scenarios():
    if not SCENARIOS_PATH.exists():
        return ValidationReport(total=0, passed_count=0, failed_count=0, items=[])
    
    try:
        scen_data = yaml.safe_load(SCENARIOS_PATH.read_text(encoding="utf-8")) or {}
        scenarios = scen_data.get("scenarios", []) or []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load scenarios: {e}")

    items = []
    passed_count = 0
    
    for s in scenarios:
        sid = s.get("id", "unknown")
        label = s.get("label", sid)
        expectations = s.get("expectations", {}) or {}
        
        # Prepare request
        gate_req = PolicyGateRequest(
            text=s.get("description") or s.get("label") or "",
            features=s.get("features", {}),
            portfolio_state=s.get("portfolio_state", {}),
            constraints={},
            top_k_router=5,
            top_k_rule_hits=5
        )
        
        try:
            # We call the existing endpoint logic internally
            res = await policy_gate(gate_req)
            
            details = []
            passed = True
            
            # Helper to check expectations
            for key, exp in expectations.items():
                op = str(exp.get("op") or "").strip()
                val = _safe_float(exp.get("value"))
                tol = _safe_float(exp.get("tol"))
                scope = str(exp.get("scope") or "").strip().lower()  # multipliers | absolute | ""
                if not op or val is None:
                    continue

                # Default: risk_multiplier is a multiplier; other guardrail keys are absolute.
                if scope == "multipliers":
                    actual = _safe_float(res.risk_overlay.multipliers.get(key))
                elif scope == "absolute":
                    actual = _safe_float(res.risk_overlay.absolute.get(key))
                else:
                    if key == "risk_multiplier":
                        actual = _safe_float(res.risk_overlay.multipliers.get(key))
                    else:
                        actual = _safe_float(res.risk_overlay.absolute.get(key))
                        if actual is None:
                            actual = _safe_float(res.risk_overlay.multipliers.get(key))

                if actual is None:
                    details.append(f"âŒ {key}: é¢„æœŸ {op} {val}, ä½†è¾“å‡ºä¸­æœªæ‰¾åˆ°è¯¥æŒ‡æ ‡")
                    passed = False
                    continue

                if _cmp_expectation(op, actual, val, tol=tol):
                    if op in ("~", "â‰ˆ", "approx"):
                        details.append(f"âœ… {key}: é¢„æœŸ {op} {val} Â± {tol if tol is not None else 0.05}, å®é™… {actual}")
                    else:
                        details.append(f"âœ… {key}: é¢„æœŸ {op} {val}, å®é™… {actual}")
                else:
                    if op in ("~", "â‰ˆ", "approx"):
                        details.append(f"âŒ {key}: é¢„æœŸ {op} {val} Â± {tol if tol is not None else 0.05}, å®é™… {actual}")
                    else:
                        details.append(f"âŒ {key}: é¢„æœŸ {op} {val}, å®é™… {actual}")
                    passed = False
            
            if passed:
                passed_count += 1
                
            items.append(ValidationItem(
                scenario_id=sid,
                label=label,
                passed=passed,
                details=details,
                results={
                    "regime": res.regime,
                    "risk_overlay": res.risk_overlay.model_dump()
                }
            ))
            
        except Exception as e:
            items.append(ValidationItem(
                scenario_id=sid,
                label=label,
                passed=False,
                details=[f"ğŸ”¥ ç³»ç»Ÿé”™è¯¯: {e}"]
            ))

    return ValidationReport(
        total=len(scenarios),
        passed_count=passed_count,
        failed_count=len(scenarios) - passed_count,
        items=items
    )


@app.post("/api/route", response_model=List[RouteResponse])
async def route(req: RouteRequest):
    if not req.text or not req.text.strip():
        raise HTTPException(status_code=400, detail="text is required")
    try:
        ranked = _route_investors(req.text, top_k=req.top_k)
        return [RouteResponse(**x) for x in ranked]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/query", response_model=List[QueryResponse])
async def query(req: QueryRequest):
    if vectorstore is None:
        raise HTTPException(status_code=503, detail="Vectorstore not ready")

    # æ„å»ºè¿‡æ»¤å™¨ (Chroma è¯­æ³•)
    filters = []
    if req.investor_id:
        filters.append({"investor_id": req.investor_id})
    if req.source_type:
        filters.append({"source_type": req.source_type})
    if req.kind:
        filters.append({"kind": req.kind})

    filter_dict = None
    if len(filters) == 1:
        filter_dict = filters[0]
    elif len(filters) > 1:
        filter_dict = {"$and": filters}

    try:
        results = query_vectorstore(vectorstore, req.query, k=req.top_k, filter_dict=filter_dict)
        
        responses = []
        for doc, score in results:
            responses.append(QueryResponse(
                content=doc.page_content,
                metadata=doc.metadata,
                similarity_estimate=round(1 - score, 4)
            ))
        return responses
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# Compatibility: keep the web frontend calling /api/rag/query
@app.post("/api/rag/query", response_model=List[QueryResponse])
async def query_alias(req: QueryRequest, authorization: Optional[str] = Header(None)):
    # Optional token for regular query as well (NOFX style)
    if os.getenv("IMH_API_TOKEN") or (authorization and "Bearer" in authorization):
        _require_bearer_token(authorization)

    return await query(req)


def _score_regimes(policy: Dict[str, Any], features: Dict[str, float]) -> Dict[str, Any]:
    regimes = policy.get("regimes", []) or []
    scores: Dict[str, float] = {}
    reasons: Dict[str, List[str]] = {}

    for r in regimes:
        rid = r.get("id")
        if not rid:
            continue
        total = 0.0
        rs: List[str] = []
        for rule in (r.get("rules", []) or []):
            feat = rule.get("feature")
            op = rule.get("op")
            val = _safe_float(rule.get("value"))
            w = _safe_float(rule.get("weight")) or 0.0
            if not feat or not op or val is None:
                continue
            fv = _safe_float(features.get(str(feat)))
            if fv is None:
                continue
            if _cmp(str(op), fv, val):
                total += w
                rs.append(f"{feat} {op} {val} (got {fv})")
        if total > 0:
            scores[rid] = total
            reasons[rid] = rs

    if not scores:
        return {"id": "neutral", "label": "ä¸­æ€§ / Neutral", "score": 0.0, "confidence": 0.0, "reasons": []}

    best_id, best_score = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[0]
    # confidence: normalized by sum
    ssum = sum(scores.values())
    conf = (best_score / ssum) if ssum > 0 else 0.0
    label = best_id
    for r in regimes:
        if r.get("id") == best_id and r.get("label"):
            label = r.get("label")
            break

    return {"id": best_id, "label": label, "score": round(best_score, 4), "confidence": round(conf, 4), "reasons": reasons.get(best_id, [])}


def _compute_overlay(policy: Dict[str, Any], regime_id: str, scenarios: List[str], portfolio_state: Dict[str, float], constraints: Dict[str, float]) -> Dict[str, Any]:
    base = (policy.get("base_guardrails", {}) or {}).copy()
    clamps = policy.get("clamps", {}) or {}

    regime_overlays = policy.get("regime_overlays", {}) or {}
    overlay = (regime_overlays.get(regime_id) or regime_overlays.get("neutral") or {}).copy()

    # Start multipliers
    multipliers: Dict[str, float] = {}
    for k in ["risk_multiplier", "max_leverage", "min_cash", "max_invest", "max_turnover", "max_corr"]:
        multipliers[k] = float(_safe_float(overlay.get(k)) or 1.0)

    # Scenario tweaks
    scen_over = policy.get("scenario_overlays", {}) or {}
    for s in scenarios:
        m = scen_over.get(s) or {}
        for k in ["max_leverage", "min_cash", "max_invest", "max_turnover", "max_corr"]:
            if k in m:
                multipliers[k] *= float(_safe_float(m.get(k)) or 1.0)

    # Portfolio pressure tweaks
    port_rules = policy.get("portfolio_overlays", {}) or {}
    for feat, rules in port_rules.items():
        pv = _safe_float(portfolio_state.get(str(feat)))
        if pv is None:
            continue
        for rule in (rules or []):
            op = str(rule.get("op") or "")
            val = _safe_float(rule.get("value"))
            if not op or val is None:
                continue
            if _cmp(op, pv, val):
                for k in ["max_leverage", "min_cash", "max_invest", "max_turnover", "max_corr"]:
                    if k in rule:
                        multipliers[k] *= float(_safe_float(rule.get(k)) or 1.0)

    # Build absolute guardrails
    absolute: Dict[str, float] = {}
    for k in ["max_leverage", "min_cash", "max_invest", "max_turnover", "max_corr"]:
        b = float(_safe_float(base.get(k)) or 0.0)
        abs_val = b * float(multipliers.get(k, 1.0))

        c = clamps.get(k) or {}
        lo = float(_safe_float(c.get("min")) or 0.0)
        hi = float(_safe_float(c.get("max")) or 1e9)
        abs_val = _clamp(abs_val, lo, hi)

        # Apply user constraints if present
        # Convention:
        # - min_* means lower bound; max_* means upper bound
        if k.startswith("min_"):
            user_min = _safe_float(constraints.get(k))
            if user_min is not None:
                abs_val = max(abs_val, user_min)
        else:
            user_max = _safe_float(constraints.get(k))
            if user_max is not None:
                abs_val = min(abs_val, user_max)

        absolute[k] = round(abs_val, 6)

    multipliers = {k: round(v, 6) for k, v in multipliers.items()}
    return {"multipliers": multipliers, "absolute": absolute}


@app.post("/api/policy/gate", response_model=PolicyGateResponse)
async def policy_gate(req: PolicyGateRequest, auto_fill_features: bool = True):
    """
    Policy Gate - è©•ä¼°å¸‚å ´ç‹€æ…‹ä¸¦æä¾›é¢¨éšªèª¿æ•´å»ºè­°
    
    Args:
        req: PolicyGateRequest (text, features, portfolio_state, constraints)
        auto_fill_features: æ˜¯å¦è‡ªå‹•å¡«å……ç¼ºå¤±çš„å¸‚å ´ç‰¹å¾µ (å¾å¯¦æ™‚æ•¸æ“š)
    """
    if not req.text or not req.text.strip():
        raise HTTPException(status_code=400, detail="text is required")

    if vectorstore is None:
        raise HTTPException(status_code=503, detail="Vectorstore not ready")
    
    # è‡ªå‹•å¡«å……ç¼ºå¤±çš„å¸‚å ´ç‰¹å¾µ
    if auto_fill_features:
        features = await _auto_fill_features(req.features or {})
        req.features = features

    policy = _load_policy()
    regime = _score_regimes(policy, req.features or {})

    scenarios = _match_scenarios(req.text)
    scenario_info = {
        "matched": scenarios,
        "primary": (scenarios[0] if scenarios else None),
        "count": len(scenarios),
    }

    router_raw = _route_investors(req.text, top_k=req.top_k_router)
    router = [RouteResponse(**x) for x in router_raw]

    overlay = _compute_overlay(policy, regime.get("id") or "neutral", scenarios, req.portfolio_state or {}, req.constraints or {})

    # RAG rule hits (source_type=rule)
    query_text = req.text
    # augment query with compact feature + portfolio summary (keeps it explainable)
    try:
        f_summary = ", ".join([f"{k}={v}" for k, v in (req.features or {}).items()][:12])
        p_summary = ", ".join([f"{k}={v}" for k, v in (req.portfolio_state or {}).items()][:12])
        if f_summary:
            query_text += f"\nFEATURES: {f_summary}"
        if p_summary:
            query_text += f"\nPORTFOLIO: {p_summary}"
    except Exception:
        pass

    try:
        results = query_vectorstore(vectorstore, query_text, k=req.top_k_rule_hits, filter_dict={"source_type": "rule"})
        rule_hits: List[EvidenceItem] = []
        for doc, score in results:
            rule_hits.append(
                EvidenceItem(
                    content=doc.page_content,
                    metadata=doc.metadata,
                    similarity_estimate=round(1 - score, 4),
                )
            )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"policy gate rag error: {e}")

    # Human-friendly narrative
    md_lines = []
    md_lines.append("## Policy Gate è¾“å‡º")
    md_lines.append(f"- Regime: **{regime.get('id')}** ({regime.get('label')})  conf={regime.get('confidence')}")
    if scenarios:
        md_lines.append(f"- Scenario: {', '.join(scenarios)}")
    else:
        md_lines.append("- Scenario: (none)")
    md_lines.append("\n## Risk Overlayï¼ˆä¸æ”¹æ–¹å‘ï¼Œåªæ”¹ä½ æ•¢ä¸‹å¤šå°‘ï¼‰")
    md_lines.append("### Multipliers")
    for k, v in overlay["multipliers"].items():
        md_lines.append(f"- {k}: {v}")
    md_lines.append("### Absolute Guardrails")
    for k, v in overlay["absolute"].items():
        md_lines.append(f"- {k}: {v}")
    md_lines.append("\n## Routerï¼ˆå»ºè®®å…ˆé—®è°ï¼‰")
    for r in router[: min(len(router), 5)]:
        md_lines.append(f"- {r.chinese_name} ({r.investor_id}): {', '.join(r.reasons[:2])}")
    md_lines.append("\n## Evidenceï¼ˆè§„åˆ™å‘½ä¸­ï¼‰")
    for i, hit in enumerate(rule_hits[: min(len(rule_hits), 6)]):
        meta = hit.metadata or {}
        md_lines.append(f"- [{i+1}] {meta.get('investor_id','?')} {meta.get('rule_id','')} {meta.get('kind','')}: {meta.get('title_hint') or meta.get('source')}")

    explanation = {
        "markdown": "\n".join(md_lines),
        "json": {
            "regime": regime,
            "scenario": scenario_info,
            "overlay": overlay,
            "router_top": [r.model_dump() for r in router[: min(len(router), 5)]],
        },
    }

    audit = {
        "ts": int(__import__("time").time()),
        "policy_hash": _policy_cache_hash,
        "input_hash": _hash_input({
            "text": req.text,
            "features": req.features,
            "portfolio_state": req.portfolio_state,
            "constraints": req.constraints,
        }),
        "regime": regime,
        "scenario": scenario_info,
    }

    # Minimal audit log (JSONL): safe, local, append-only
    try:
        AUDIT_DIR.mkdir(parents=True, exist_ok=True)
        record = {
            **audit,
            "overlay": overlay,
            "router": [r.model_dump() for r in router[: min(len(router), 10)]],
            "rule_hits_meta": [
                {
                    "investor_id": (h.metadata or {}).get("investor_id"),
                    "rule_id": (h.metadata or {}).get("rule_id"),
                    "kind": (h.metadata or {}).get("kind"),
                    "source": (h.metadata or {}).get("source"),
                    "title_hint": (h.metadata or {}).get("title_hint"),
                    "similarity_estimate": h.similarity_estimate,
                }
                for h in rule_hits[: min(len(rule_hits), 20)]
            ],
        }
        with AUDIT_PATH.open("a", encoding="utf-8") as f:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    except Exception:
        # Never fail trading/analytics because logging failed
        pass

    return PolicyGateResponse(
        regime=regime,
        scenario=scenario_info,
        router=router,
        rule_hits=rule_hits,
        risk_overlay=RiskOverlay(**overlay),
        explanation=explanation,
        audit=audit,
    )


# ============================================
# åé¥‹ API ç«¯é»
# ============================================
class FeedbackRequest(BaseModel):
    """åé¥‹è«‹æ±‚"""
    session_id: str
    query: str
    response_id: str
    feedback_type: str  # "thumbs_up", "thumbs_down", "rating"
    rating: Optional[int] = None  # 1-5 åˆ†
    comment: Optional[str] = None


class FeedbackResponse(BaseModel):
    """åé¥‹éŸ¿æ‡‰"""
    success: bool
    feedback_id: str
    message: str


@app.post("/api/feedback", response_model=FeedbackResponse)
async def submit_feedback(req: FeedbackRequest):
    """
    æäº¤ç”¨æˆ¶åé¥‹
    
    Args:
        req: FeedbackRequest (session_id, query, response_id, feedback_type, rating, comment)
    
    Returns:
        FeedbackResponse (success, feedback_id, message)
    """
    try:
        collector = get_feedback_collector()
        
        record = collector.submit_feedback(
            session_id=req.session_id,
            query=req.query,
            response_id=req.response_id,
            feedback_type=req.feedback_type,
            rating=req.rating,
            comment=req.comment
        )
        
        return FeedbackResponse(
            success=True,
            feedback_id=record["id"],
            message="åé¥‹å·²ä¿å­˜"
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åé¥‹æäº¤å¤±æ•—ï¼š{e}")


@app.get("/api/feedback/stats", response_model=Dict[str, Any])
async def get_feedback_stats(days: int = 7):
    """
    ç²å–åé¥‹çµ±è¨ˆæ•¸æ“š
    
    Args:
        days: çµ±è¨ˆå¤©æ•¸
    
    Returns:
        çµ±è¨ˆæ•¸æ“šå­—å…¸ (total_feedback, average_rating, nps, thumbs_up_ratio, etc.)
    """
    try:
        analyzer = get_feedback_analyzer()
        stats = analyzer.analyze(days=days)
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç²å–çµ±è¨ˆå¤±æ•—ï¼š{e}")


@app.get("/api/feedback/report")
async def get_feedback_report(days: int = 7):
    """
    ç²å–åé¥‹åˆ†æå ±å‘Š
    
    Args:
        days: å ±å‘Šå¤©æ•¸
    
    Returns:
        æ–‡æœ¬å ±å‘Š
    """
    try:
        analyzer = get_feedback_analyzer()
        report = analyzer.generate_report(days=days)
        return {"report": report, "days": days}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå ±å‘Šå¤±æ•—ï¼š{e}")


@app.get("/")
async def web_index():
    """
    Serve the exported static web app.
    """
    index_file = WEB_OUT_DIR / "index.html"
    if not index_file.exists():
        raise HTTPException(
            status_code=404,
            detail="Web UI not built. Run: cd web && npm install && npm run build",
        )
    return FileResponse(index_file)


# Mount static after API routes so /query stays functional.
if WEB_OUT_DIR.exists():
    # Support both hosting styles:
    # - root:   http://host:port/
    # - /imh:   http://host:port/imh/  (when integrated into another app or basePath is used)
    #
    # IMPORTANT: mount /imh BEFORE /, otherwise / will swallow /imh and cause 404 for /imh/* assets.
    app.mount("/imh", StaticFiles(directory=str(WEB_OUT_DIR), html=True), name="web_imh")
    app.mount("/", StaticFiles(directory=str(WEB_OUT_DIR), html=True), name="web")

# ============================================
# å¯¦æ™‚æ•¸æ“šé›†æˆ
# ============================================
async def _auto_fill_features(features: Dict[str, float]) -> Dict[str, float]:
    """
    è‡ªå‹•å¡«å……ç¼ºå¤±çš„å¸‚å ´ç‰¹å¾µ
    
    å¾å¯¦æ™‚æ•¸æ“šç®¡é“ç²å–ç¼ºå¤±çš„é—œéµæŒ‡æ¨™:
    - vix: å¸‚å ´æ³¢å‹•ç‡
    - inflation: é€šè†¨ç‡
    - rates: è¯é‚¦åŸºé‡‘åˆ©ç‡
    - treasury_10y: 10 å¹´æœŸåœ‹å‚µæ”¶ç›Šç‡
    - sp500_pe_ratio: S&P500 æœ¬ç›Šæ¯”
    
    Args:
        features: ç”¨æˆ¶æä¾›çš„ç‰¹å¾µ
    
    Returns:
        å¡«å……å¾Œçš„ç‰¹å¾µå­—å…¸
    """
    # éœ€è¦å¡«å……çš„å­—æ®µ
    required_fields = ["vix", "inflation", "rates", "treasury_10y", "sp500_pe_ratio"]
    missing_fields = [f for f in required_fields if f not in features]
    
    if not missing_fields:
        # æ²’æœ‰ç¼ºå¤±å­—æ®µï¼Œç›´æ¥è¿”å›
        return features
    
    try:
        # ç²å–å¯¦æ™‚æ•¸æ“š
        from .realtime_data import get_pipeline
        pipeline = get_pipeline()
        await pipeline.start()
        
        try:
            realtime_features = await pipeline.get_all_features()
            
            # åªå¡«å……ç¼ºå¤±çš„å­—æ®µ (ä¸è¦†è“‹ç”¨æˆ¶æä¾›çš„å€¼)
            for field in missing_fields:
                if field in realtime_features:
                    features[field] = realtime_features[field]
                    print(f"âœ… è‡ªå‹•å¡«å…… {field}: {realtime_features[field]}")
            
            return features
            
        finally:
            await pipeline.stop()
            
    except Exception as e:
        print(f"âš ï¸ è‡ªå‹•å¡«å……ç‰¹å¾µå¤±æ•—ï¼š{e}")
        # é™ç´šï¼šè¿”å›åŸå§‹ç‰¹å¾µ (ä¸å¡«å……)
        return features


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "8001"))
    uvicorn.run(app, host="0.0.0.0", port=port)
